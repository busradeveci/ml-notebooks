{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11589020,"sourceType":"datasetVersion","datasetId":7266777}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### In this project, I worked on a Kaggle fake news dataset using both classical machine learning methods (TF-IDF with Logistic Regression and Naive Bayes) and deep learning (BERT). Throughout the process, I focused on data preprocessing, feature extraction, modeling, and evaluation. While the results did not reach high accuracy levels (~50%), the experience provided valuable insights into the challenges of working with real-world text data — such as limited vocabulary diversity, label reliability, and high contextual similarity between classes. This notebook summarizes each step taken, the observations made, and the key takeaways from this learning-focused project. It was a great opportunity to gain hands-on experience in data analysis, machine learning pipelines, and model diagnostics.","metadata":{}},{"cell_type":"code","source":"# Gerekli kütüphaneleri yükleme\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datasets import Dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nimport torch\nimport transformers\n\n# NLTK veri setlerini indir\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:28:47.707765Z","iopub.execute_input":"2025-06-03T22:28:47.708034Z","iopub.status.idle":"2025-06-03T22:28:47.716790Z","shell.execute_reply.started":"2025-06-03T22:28:47.708016Z","shell.execute_reply":"2025-06-03T22:28:47.715666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Veri setini yükle\ndf = pd.read_csv('/kaggle/input/fake-news-detection-dataset/fake_news_dataset.csv') \nprint(df.head())\nprint(df.info())\nprint(df['label'].value_counts())\nprint(df[['text', 'label']].isna().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:16:29.906377Z","iopub.execute_input":"2025-06-04T19:16:29.906715Z","iopub.status.idle":"2025-06-04T19:16:29.940558Z","shell.execute_reply.started":"2025-06-04T19:16:29.906691Z","shell.execute_reply":"2025-06-04T19:16:29.939111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Stop kelimeleri yükle, negatif kelimeleri koru\nstop_words = set(stopwords.words('english')) - {'not', 'no', 'nor'}\nlemmatizer = WordNetLemmatizer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:28:48.533447Z","iopub.execute_input":"2025-06-03T22:28:48.533727Z","iopub.status.idle":"2025-06-03T22:28:48.540618Z","shell.execute_reply.started":"2025-06-03T22:28:48.533700Z","shell.execute_reply":"2025-06-03T22:28:48.539879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ön işleme fonksiyonu\ndef preprocess_text(text):\n    if isinstance(text, str):\n        tokens = word_tokenize(text.lower())  # Küçük harfe çevir ve tokenize et\n        tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n        return ' '.join(tokens)\n    return ''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:28:48.541497Z","iopub.execute_input":"2025-06-03T22:28:48.541766Z","iopub.status.idle":"2025-06-03T22:28:48.554841Z","shell.execute_reply.started":"2025-06-03T22:28:48.541741Z","shell.execute_reply":"2025-06-03T22:28:48.553906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Metin sütununa uygula\ndf['clean_text'] = df['text'].apply(preprocess_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:28:48.557344Z","iopub.execute_input":"2025-06-03T22:28:48.557687Z","iopub.status.idle":"2025-06-03T22:29:21.216903Z","shell.execute_reply.started":"2025-06-03T22:28:48.557667Z","shell.execute_reply":"2025-06-03T22:29:21.215966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Eksik verileri kontrol et ve temizle\ndf = df[df['clean_text'] != '']  # Boş metinleri kaldır\nprint(df['clean_text'].head())\nprint(df.shape)\nprint(df['clean_text'].apply(lambda x: len(x.split())).describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:29:21.217816Z","iopub.execute_input":"2025-06-03T22:29:21.218043Z","iopub.status.idle":"2025-06-03T22:29:21.450309Z","shell.execute_reply.started":"2025-06-03T22:29:21.218026Z","shell.execute_reply":"2025-06-03T22:29:21.449564Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TF-IDF vektörleştirme\nvectorizer = TfidfVectorizer(max_features=10000)\nX = vectorizer.fit_transform(df['clean_text'])\ny = df['label'].map({'fake': 0, 'real': 1})  # Etiketleri sayısal yap: fake=0, real=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:29:21.451049Z","iopub.execute_input":"2025-06-03T22:29:21.451355Z","iopub.status.idle":"2025-06-03T22:29:24.066050Z","shell.execute_reply.started":"2025-06-03T22:29:21.451328Z","shell.execute_reply":"2025-06-03T22:29:24.065209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train/test böl\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(X_train.shape, X_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:29:24.066882Z","iopub.execute_input":"2025-06-03T22:29:24.067230Z","iopub.status.idle":"2025-06-03T22:29:24.087787Z","shell.execute_reply.started":"2025-06-03T22:29:24.067202Z","shell.execute_reply":"2025-06-03T22:29:24.086925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Logistic Regression modeli\nlr_model = LogisticRegression(max_iter=1000)\nlr_model.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:29:24.088619Z","iopub.execute_input":"2025-06-03T22:29:24.088915Z","iopub.status.idle":"2025-06-03T22:29:24.291688Z","shell.execute_reply.started":"2025-06-03T22:29:24.088875Z","shell.execute_reply":"2025-06-03T22:29:24.290816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tahmin ve değerlendirme\ny_pred = lr_model.predict(X_test)\nprint('Logistic Regression Accuracy:', accuracy_score(y_test, y_pred))\nprint('Logistic Regression F1-Score:', f1_score(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:29:24.292374Z","iopub.execute_input":"2025-06-03T22:29:24.292643Z","iopub.status.idle":"2025-06-03T22:29:24.312085Z","shell.execute_reply.started":"2025-06-03T22:29:24.292620Z","shell.execute_reply":"2025-06-03T22:29:24.311189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Logistic Regression Confusion Matrix')\nplt.xlabel('Tahmin Edilen')\nplt.ylabel('Gerçek')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:29:24.317699Z","iopub.execute_input":"2025-06-03T22:29:24.318058Z","iopub.status.idle":"2025-06-03T22:29:24.570359Z","shell.execute_reply.started":"2025-06-03T22:29:24.318031Z","shell.execute_reply":"2025-06-03T22:29:24.569420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\nfrom transformers import BertTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:29:24.571581Z","iopub.execute_input":"2025-06-03T22:29:24.571834Z","iopub.status.idle":"2025-06-03T22:29:24.575584Z","shell.execute_reply.started":"2025-06-03T22:29:24.571813Z","shell.execute_reply":"2025-06-03T22:29:24.574801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pandas’tan Hugging Face Dataset’e çevir\ndata_dict = {'text': df['text'], 'labels': df['label'].map({'fake': 0, 'real': 1})}\ndataset = Dataset.from_dict(data_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:29:24.576683Z","iopub.execute_input":"2025-06-03T22:29:24.577055Z","iopub.status.idle":"2025-06-03T22:29:24.943098Z","shell.execute_reply.started":"2025-06-03T22:29:24.577028Z","shell.execute_reply":"2025-06-03T22:29:24.942218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train/test böl\ntrain_test = dataset.train_test_split(test_size=0.2, seed=42)\ntrain_dataset = train_test['train']\ntest_dataset = train_test['test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:29:24.944198Z","iopub.execute_input":"2025-06-03T22:29:24.944619Z","iopub.status.idle":"2025-06-03T22:29:24.975874Z","shell.execute_reply.started":"2025-06-03T22:29:24.944586Z","shell.execute_reply":"2025-06-03T22:29:24.975011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:29:24.976935Z","iopub.execute_input":"2025-06-03T22:29:24.977277Z","iopub.status.idle":"2025-06-03T22:29:27.779763Z","shell.execute_reply.started":"2025-06-03T22:29:24.977247Z","shell.execute_reply":"2025-06-03T22:29:27.779086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenize fonksiyonu\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:29:27.780497Z","iopub.execute_input":"2025-06-03T22:29:27.780715Z","iopub.status.idle":"2025-06-03T22:29:27.785002Z","shell.execute_reply.started":"2025-06-03T22:29:27.780698Z","shell.execute_reply":"2025-06-03T22:29:27.784237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Veriyi tokenize et\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:29:27.785962Z","iopub.execute_input":"2025-06-03T22:29:27.786254Z","iopub.status.idle":"2025-06-03T22:30:49.412728Z","shell.execute_reply.started":"2025-06-03T22:29:27.786232Z","shell.execute_reply":"2025-06-03T22:30:49.411979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset formatını ayarla\ntrain_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\ntest_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:30:49.413542Z","iopub.execute_input":"2025-06-03T22:30:49.413861Z","iopub.status.idle":"2025-06-03T22:30:49.419899Z","shell.execute_reply.started":"2025-06-03T22:30:49.413833Z","shell.execute_reply":"2025-06-03T22:30:49.419112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Etiketleri kontrol et\nprint(train_dataset[0]['labels'])  # 0 veya 1 (integer) olmalı\nprint(type(train_dataset[0]['labels']))  # <class 'torch.Tensor'> olmalı","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:30:49.420787Z","iopub.execute_input":"2025-06-03T22:30:49.421047Z","iopub.status.idle":"2025-06-03T22:30:49.445661Z","shell.execute_reply.started":"2025-06-03T22:30:49.421031Z","shell.execute_reply":"2025-06-03T22:30:49.444853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Modeli yükle\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:30:49.446496Z","iopub.execute_input":"2025-06-03T22:30:49.446824Z","iopub.status.idle":"2025-06-03T22:30:53.300805Z","shell.execute_reply.started":"2025-06-03T22:30:49.446798Z","shell.execute_reply":"2025-06-03T22:30:53.299943Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Eğitim parametreleri\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    logging_dir='./logs',\n    logging_steps=100,\n    report_to='none',  \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:54:58.229000Z","iopub.execute_input":"2025-06-03T22:54:58.229307Z","iopub.status.idle":"2025-06-03T22:54:58.235925Z","shell.execute_reply.started":"2025-06-03T22:54:58.229287Z","shell.execute_reply":"2025-06-03T22:54:58.234565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Metrik hesaplama fonksiyonu\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds)\n    return {'accuracy': acc, 'f1': f1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:55:09.889563Z","iopub.execute_input":"2025-06-03T22:55:09.890417Z","iopub.status.idle":"2025-06-03T22:55:09.895455Z","shell.execute_reply.started":"2025-06-03T22:55:09.890386Z","shell.execute_reply":"2025-06-03T22:55:09.894206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Trainer oluştur\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:55:28.469343Z","iopub.execute_input":"2025-06-03T22:55:28.469633Z","iopub.status.idle":"2025-06-03T22:55:28.485743Z","shell.execute_reply.started":"2025-06-03T22:55:28.469609Z","shell.execute_reply":"2025-06-03T22:55:28.484879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Modeli eğit\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T22:55:35.884138Z","iopub.execute_input":"2025-06-03T22:55:35.884494Z","iopub.status.idle":"2025-06-04T08:40:03.867997Z","shell.execute_reply.started":"2025-06-03T22:55:35.884469Z","shell.execute_reply":"2025-06-04T08:40:03.865027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Gerekli kütüphaneler\nimport pandas as pd\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:19:59.267678Z","iopub.execute_input":"2025-06-04T19:19:59.268048Z","iopub.status.idle":"2025-06-04T19:19:59.272728Z","shell.execute_reply.started":"2025-06-04T19:19:59.268028Z","shell.execute_reply":"2025-06-04T19:19:59.271755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Veri setini yükle\ndf = pd.read_csv('/kaggle/input/fake-news-detection-dataset/fake_news_dataset.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:20:16.798003Z","iopub.execute_input":"2025-06-04T19:20:16.798376Z","iopub.status.idle":"2025-06-04T19:20:17.980115Z","shell.execute_reply.started":"2025-06-04T19:20:16.798351Z","shell.execute_reply":"2025-06-04T19:20:17.978975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sahte ve gerçek metinler\nfake_text = ' '.join(df[df['label'] == 'fake']['text'])\nreal_text = ' '.join(df[df['label'] == 'real']['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:20:24.781811Z","iopub.execute_input":"2025-06-04T19:20:24.782141Z","iopub.status.idle":"2025-06-04T19:20:24.841694Z","shell.execute_reply.started":"2025-06-04T19:20:24.782118Z","shell.execute_reply":"2025-06-04T19:20:24.840880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sahte haberler kelime bulutu\nfake_wc = WordCloud(width=800, height=400, max_words=100).generate(fake_text)\nplt.figure(figsize=(10, 5))\nplt.imshow(fake_wc, interpolation='bilinear')\nplt.title('Sahte Haberler Kelime Bulutu')\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:20:31.047167Z","iopub.execute_input":"2025-06-04T19:20:31.047575Z","iopub.status.idle":"2025-06-04T19:20:43.009995Z","shell.execute_reply.started":"2025-06-04T19:20:31.047552Z","shell.execute_reply":"2025-06-04T19:20:43.008832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Gerçek haberler kelime bulutu\nreal_wc = WordCloud(width=800, height=400, max_words=100).generate(real_text)\nplt.figure(figsize=(10, 5))\nplt.imshow(real_wc, interpolation='bilinear')\nplt.title('Gerçek Haberler Kelime Bulutu')\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:21:02.308682Z","iopub.execute_input":"2025-06-04T19:21:02.308982Z","iopub.status.idle":"2025-06-04T19:21:14.081156Z","shell.execute_reply.started":"2025-06-04T19:21:02.308961Z","shell.execute_reply":"2025-06-04T19:21:14.079757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:30:59.439621Z","iopub.execute_input":"2025-06-04T19:30:59.439941Z","iopub.status.idle":"2025-06-04T19:30:59.463111Z","shell.execute_reply.started":"2025-06-04T19:30:59.439918Z","shell.execute_reply":"2025-06-04T19:30:59.462060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sahte ve gerçek metinler\nfake_texts = df[df['label'] == 'fake']['text']\nreal_texts = df[df['label'] == 'real']['text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:31:00.507159Z","iopub.execute_input":"2025-06-04T19:31:00.507861Z","iopub.status.idle":"2025-06-04T19:31:00.527380Z","shell.execute_reply.started":"2025-06-04T19:31:00.507792Z","shell.execute_reply":"2025-06-04T19:31:00.526160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CountVectorizer ile kelime frekanslarını hesapla\nvectorizer = CountVectorizer(stop_words='english', max_features=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:31:01.123427Z","iopub.execute_input":"2025-06-04T19:31:01.123728Z","iopub.status.idle":"2025-06-04T19:31:01.129166Z","shell.execute_reply.started":"2025-06-04T19:31:01.123704Z","shell.execute_reply":"2025-06-04T19:31:01.128091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sahte haberler için\nfake_vectorizer = vectorizer.fit(fake_texts)\nfake_freq = fake_vectorizer.transform(fake_texts)\nfake_words = fake_vectorizer.get_feature_names_out()\nfake_counts = fake_freq.sum(axis=0).A1\nfake_word_freq = dict(zip(fake_words, fake_counts))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:31:14.341374Z","iopub.execute_input":"2025-06-04T19:31:14.341719Z","iopub.status.idle":"2025-06-04T19:31:17.764208Z","shell.execute_reply.started":"2025-06-04T19:31:14.341693Z","shell.execute_reply":"2025-06-04T19:31:17.762404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Gerçek haberler için\nreal_vectorizer = vectorizer.fit(real_texts)\nreal_freq = real_vectorizer.transform(real_texts)\nreal_words = real_vectorizer.get_feature_names_out()\nreal_counts = real_freq.sum(axis=0).A1\nreal_word_freq = dict(zip(real_words, real_counts))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:31:22.850672Z","iopub.execute_input":"2025-06-04T19:31:22.851011Z","iopub.status.idle":"2025-06-04T19:31:26.161753Z","shell.execute_reply.started":"2025-06-04T19:31:22.850987Z","shell.execute_reply":"2025-06-04T19:31:26.160735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sonuçları yazdır\nprint(\"Sahte Haberlerde En Sık 20 Kelime:\")\nfor word, count in sorted(fake_word_freq.items(), key=lambda x: x[1], reverse=True):\n    print(f\"{word}: {count}\")\n\nprint(\"\\nGerçek Haberlerde En Sık 20 Kelime:\")\nfor word, count in sorted(real_word_freq.items(), key=lambda x: x[1], reverse=True):\n    print(f\"{word}: {count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:31:36.742038Z","iopub.execute_input":"2025-06-04T19:31:36.742405Z","iopub.status.idle":"2025-06-04T19:31:36.750290Z","shell.execute_reply.started":"2025-06-04T19:31:36.742381Z","shell.execute_reply":"2025-06-04T19:31:36.749077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:36:04.994067Z","iopub.execute_input":"2025-06-04T19:36:04.994453Z","iopub.status.idle":"2025-06-04T19:36:05.270100Z","shell.execute_reply.started":"2025-06-04T19:36:04.994427Z","shell.execute_reply":"2025-06-04T19:36:05.269027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Kısa metinleri çıkar\ndf = df[df['text'].str.split().str.len() > 50]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:36:05.725946Z","iopub.execute_input":"2025-06-04T19:36:05.726287Z","iopub.status.idle":"2025-06-04T19:36:06.430968Z","shell.execute_reply.started":"2025-06-04T19:36:05.726266Z","shell.execute_reply":"2025-06-04T19:36:06.429902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TF-IDF ve basit model\nvectorizer = TfidfVectorizer(max_features=5000)\nX = vectorizer.fit_transform(df['text'])\ny = df['label'].map({'fake': 0, 'real': 1})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:36:06.432542Z","iopub.execute_input":"2025-06-04T19:36:06.432900Z","iopub.status.idle":"2025-06-04T19:36:10.272742Z","shell.execute_reply.started":"2025-06-04T19:36:06.432871Z","shell.execute_reply":"2025-06-04T19:36:10.271592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Logistic Regression ile güvenilirlik tahmini\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X, y)\nprobs = model.predict_proba(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:36:10.274391Z","iopub.execute_input":"2025-06-04T19:36:10.274694Z","iopub.status.idle":"2025-06-04T19:36:10.989175Z","shell.execute_reply.started":"2025-06-04T19:36:10.274672Z","shell.execute_reply":"2025-06-04T19:36:10.988456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Düşük güvenilirlikli örnekleri çıkar (olasılık 0.4-0.6 arası)\nmask = ~((probs[:, 0] > 0.4) & (probs[:, 0] < 0.6))\ndf_clean = df[mask]\n\nprint(\"Temizlenmiş veri boyutu:\", df_clean.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:36:19.788273Z","iopub.execute_input":"2025-06-04T19:36:19.788549Z","iopub.status.idle":"2025-06-04T19:36:19.797171Z","shell.execute_reply.started":"2025-06-04T19:36:19.788531Z","shell.execute_reply":"2025-06-04T19:36:19.796138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from textblob import TextBlob\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:36:34.738568Z","iopub.execute_input":"2025-06-04T19:36:34.739010Z","iopub.status.idle":"2025-06-04T19:36:35.504360Z","shell.execute_reply.started":"2025-06-04T19:36:34.738977Z","shell.execute_reply":"2025-06-04T19:36:35.503418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Duygusal ton hesapla\ndf['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:36:52.999126Z","iopub.execute_input":"2025-06-04T19:36:52.999462Z","iopub.status.idle":"2025-06-04T19:37:19.225200Z","shell.execute_reply.started":"2025-06-04T19:36:52.999438Z","shell.execute_reply":"2025-06-04T19:37:19.224173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sahte ve gerçek haberlerin ortalama sentimentini karşılaştır\nprint(\"Sahte Haberler Ortalama Sentiment:\", df[df['label'] == 'fake']['sentiment'].mean())\nprint(\"Gerçek Haberler Ortalama Sentiment:\", df[df['label'] == 'real']['sentiment'].mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:37:23.372862Z","iopub.execute_input":"2025-06-04T19:37:23.373171Z","iopub.status.idle":"2025-06-04T19:37:23.391948Z","shell.execute_reply.started":"2025-06-04T19:37:23.373152Z","shell.execute_reply":"2025-06-04T19:37:23.390519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sahte ve gerçek haberlerden rastgele 3 örnek\nprint(\"Sahte Haber Örnekleri:\")\nfor text in df[df['label'] == 'fake']['text'].sample(3, random_state=42):\n    print(f\"- {text[:200]}...\\n\")\n\nprint(\"Gerçek Haber Örnekleri:\")\nfor text in df[df['label'] == 'real']['text'].sample(3, random_state=42):\n    print(f\"- {text[:200]}...\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:39:25.729179Z","iopub.execute_input":"2025-06-04T19:39:25.729545Z","iopub.status.idle":"2025-06-04T19:39:25.750473Z","shell.execute_reply.started":"2025-06-04T19:39:25.729520Z","shell.execute_reply":"2025-06-04T19:39:25.749533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Özet\nprint(\"Veri Seti Özeti:\")\nprint(f\"- Toplam örnek: {len(df)}\")\nprint(f\"- Temizlenmiş örnek: 5721\")\nprint(f\"- Sahte haber sentiment: 0.094\")\nprint(f\"- Gerçek haber sentiment: 0.093\")\nprint(\"- Sorun: Gürültülü etiketler ve bağlamsal benzerlik. Kelime bulutlarında 'new', 'mr' gibi ortak kelimeler, BERT %48 accuracy.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T19:39:57.008687Z","iopub.execute_input":"2025-06-04T19:39:57.009092Z","iopub.status.idle":"2025-06-04T19:39:57.015357Z","shell.execute_reply.started":"2025-06-04T19:39:57.009069Z","shell.execute_reply":"2025-06-04T19:39:57.014151Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sahte Haber Tespiti Projesi: Özet ve Değerlendirme\n\n### Bu projede, Kaggle’daki bir sahte haber veri setiyle sahte ve gerçek haberleri sınıflandırmaya çalıştık. Amacımız, makine öğrenmesi (TF-IDF) ve derin öğrenme (BERT) modelleriyle yüksek doğruluk elde etmekti. Ancak veri setinin kalitesi nedeniyle beklenen sonuçları alamadık. Aşağıda, yaptığımız adımları, sonuçları ve öğrendiklerimizi özetledik.\n\n## 1. Veri Seti Yükleme ve İnceleme\n- **Ne Yaptık?**: `/kaggle/input/fake-news-detection-dataset/fake_news.csv` veri setini yükledik. 20,000 örnekten oluşan veri setinde `text` (haber metni) ve `label` (sahte/gerçek) sütunları vardı.\n- **Neden Yaptık?**: Veri setinin yapısını anlamak ve sahte/gerçek haberlerin dağılımını kontrol etmek için.\n- **Sonuç**: Veri seti dengeliydi (yaklaşık 10,000 sahte, 10,000 gerçek), ama metinlerin kalitesi şüpheliydi.\n\n## 2. TF-IDF ile Makine Öğrenmesi\n- **Ne Yaptık?**: Metinleri TF-IDF ile sayısal vektörlere çevirdik (`max_features=10000`), Logistic Regression, Naive Bayes gibi modeller denedik.\n- **Neden Yaptık?**: Basit makine öğrenmesi modelleriyle hızlı sonuç almak için.\n- **Sonuç**: TF-IDF sadece 870 özellik üretti, modeller ~%50 doğruluk verdi. Bu, veri setindeki kelime dağarcığının sınırlı olduğunu gösterdi.\n\n## 3. BERT ile Derin Öğrenme\n- **Ne Yaptık?**: BERT (`bert-base-uncased`) modelini 16,000 örnekle 3 epoch eğittik (batch_size=8, Kaggle P100 GPU). Eğitim parametreleri: `TrainingArguments` ile `eval_strategy='epoch'`, `report_to='none'`.\n- **Neden Yaptık?**: BERT, metinlerin bağlamını yakalayarak daha iyi sonuç verebilirdi.\n- **Sonuç**: 9 saat süren eğitim sonrası %48 doğruluk ve ~0.65 F1-score aldık. Model rastgele tahmin seviyesindeydi, veri setinin ayrım yapmaya uygun olmadığını gösterdi.\n- **Sorunlar**:\n  - `NameError: TrainingArguments`, `wandb.init()` gibi hatalar çıktı, `report_to='none'` ile çözdük.\n  - Checkpoint’ler (`./results/checkpoint-6000`) Kaggle’da kayboldu, `HFValidationError` aldık.\n\n## 4. Veri Seti Analizi\n- **Ne Yaptık?**:\n  - **Kelime Bulutları**: Sahte ve gerçek haberlerin kelime bulutlarını oluşturduk. `new`, `mr` gibi ortak kelimeler çıktı.\n  - **Kelime Frekansları**: `CountVectorizer` ile en sık 20 kelimeyi listeledik (sahte: `summer`, `wide`; gerçek: `economy`, `member`). Kelimeler farklıydı, ama bağlamsal ayrım için yetersizdi.\n  - **Sentiment Analizi**: `TextBlob` ile duygusal ton hesapladık (sahte: 0.094, gerçek: 0.093). Skorlar çok yakındı, ayrım yapılamadı.\n  - **Veri Temizleme**: Düşük güvenilirlikli örnekleri çıkardık, veri 5,721’e düştü.\n- **Neden Yaptık?**: Veri setinin neden başarısız olduğunu anlamak için.\n- **Sonuç**: Veri setinde gürültülü etiketler ve bağlamsal benzerlik vardı. Metinler, sahte/gerçek ayrımı için yeterince farklı değildi.\n\n## 5. Örnek Metinler\n- **Sahte Haber Örnekleri**:\n  - `where free section small present stage couple memory bag would real protect page notice...`\n  - `or onto strategy first camera stage really almost beautiful whole land thus care...`\n  - `executive reflect this family hard drive summer author direction source option help...`\n- **Gerçek Haber Örnekleri**:\n  - `ready best rich computer choose middle center expert several rich of nearly voice...`\n  - `free themselves keep bill final inside all federal popular serious claim nearly...`\n  - `seat reach TV sometimes population treatment ability until outside card case several...`\n- **Yorum**: Metinler benzer kelimeler içeriyor, ayrım yapmak zor.\n\n## 6. Öğrendiklerimiz\n- **Veri Kalitesi**: Sahte haber tespiti için veri setinin etiket kalitesi ve metinlerin ayırt ediciliği kritik.\n- **Model Sınırları**: TF-IDF ve BERT, kaliteli veri olmadan iyi sonuç vermiyor.\n- **Analiz Teknikleri**: Kelime bulutları, frekans analizi ve sentiment analizi veri seti teşhisinde etkili.\n- **Kaggle Deneyimi**: Uzun süren eğitimlerde checkpoint’leri kaydetme ve Kernel zaman aşımı sorunlarını öğrendik.\n\n## 7. Neden Başarısız Olduk?\n- **Gürültülü Etiketler**: Sahte/gerçek etiketleri yanlış veya tutarsız olabilir.\n- **Bağlamsal Benzerlik**: Metinler aynı konuları benzer kelimelerle anlatıyor (ör. `new`, `mr`).\n- **Düşük Kelime Çeşitliliği**: TF-IDF sadece 870 özellik üretti, veri seti sınırlı.\n\n## 8. Sonuç ve Kapanış\n- Bu veri setiyle sahte haber tespiti yapmak mümkün olmadı. 9 saatlik BERT denemesi, TF-IDF modelleri ve veri analizleri veri setinin yetersiz olduğunu gösterdi.\n- **Karar**: Veri setini kapatıyoruz. Yeni bir veri seti (ör. LIAR) ile denemek daha mantıklı olurdu, ama bu projeyi burada sonlandırıyoruz.\n- **Teşekkür**: Bu süreçte veri analizi, makine öğrenmesi ve derin öğrenme konusunda büyük tecrübe kazandım.\n","metadata":{}},{"cell_type":"markdown","source":"# Fake News Detection Project: Summary and Evaluation\n\n### In this project, we attempted to classify fake and real news using a dataset from Kaggle. Our goal was to achieve high accuracy using machine learning (TF-IDF) and deep learning (BERT) models. However, due to the dataset's quality, we couldn’t obtain satisfactory results. Below, we summarize the steps, outcomes, and key takeaways.\n\n## 1. Data Loading and Exploration\n- **What We Did**: Loaded the dataset from `/kaggle/input/fake-news-detection-dataset/fake_news.csv`. It contained 20,000 samples with `text` (news content) and `label` (fake/real) columns.\n- **Why**: To understand the dataset structure and check the distribution of fake/real news.\n- **Result**: The dataset was balanced (~10,000 fake, 10,000 real), but the text quality raised concerns.\n\n## 2. TF-IDF with Machine Learning\n- **What We Did**: Converted texts to numerical vectors using TF-IDF (`max_features=10000`) and tested models like Logistic Regression and Naive Bayes.\n- **Why**: To obtain quick results with simple machine learning models.\n- **Result**: TF-IDF produced only 870 features, and models achieved ~50% accuracy, indicating limited vocabulary diversity in the dataset.\n\n## 3. BERT with Deep Learning\n- **What We Did**: Trained a BERT model (`bert-base-uncased`) on 16,000 samples for 3 epochs (batch_size=8, Kaggle P100 GPU). Training parameters: `TrainingArguments` with `eval_strategy='epoch'`, `report_to='none'`.\n- **Why**: BERT could potentially capture text context for better results.\n- **Result**: After 9 hours of training, we achieved 48% accuracy and ~0.65 F1-score. The model performed at random-guess level, confirming the dataset’s unsuitability for classification.\n- **Issues**:\n  - Encountered errors like `NameError: TrainingArguments` and `wandb.init()`, resolved with `report_to='none'`.\n  - Checkpoints (`./results/checkpoint-6000`) were lost in Kaggle, leading to `HFValidationError`.\n\n## 4. Dataset Analysis\n- **What We Did**:\n  - **Word Clouds**: Generated word clouds for fake and real news. Common words like `new`, `mr` appeared.\n  - **Word Frequencies**: Used `CountVectorizer` to list the top 20 words (fake: `summer`, `wide`; real: `economy`, `member`). Words were distinct but insufficient for contextual separation.\n  - **Sentiment Analysis**: Calculated sentiment using `TextBlob` (fake: 0.094, real: 0.093). Scores were nearly identical, indicating no emotional distinction.\n  - **Data Cleaning**: Removed low-confidence samples, reducing the dataset to 5,721 samples.\n- **Why**: To diagnose why the dataset failed to support classification.\n- **Result**: The dataset had noisy labels and contextual similarity. Texts were not distinct enough for fake/real separation.\n\n## 5. Sample Texts\n- **Fake News Examples**:\n  - `where free section small present stage couple memory bag would real protect page notice...`\n  - `or onto strategy first camera stage really almost beautiful whole land thus care...`\n  - `executive reflect this family hard drive summer author direction source option help...`\n- **Real News Examples**:\n  - `ready best rich computer choose middle center expert several rich of nearly voice...`\n  - `free themselves keep bill final inside all federal popular serious claim nearly...`\n  - `seat reach TV sometimes population treatment ability until outside card case several...`\n- **Comment**: Texts share similar vocabulary, making differentiation challenging.\n\n## 6. Lessons Learned\n- **Data Quality**: The quality of labels and text distinctiveness is critical for fake news detection.\n- **Model Limitations**: TF-IDF and BERT require high-quality data to perform well.\n- **Analysis Techniques**: Word clouds, frequency analysis, and sentiment analysis are effective for dataset diagnosis.\n- **Kaggle Experience**: Learned to manage checkpoints and handle Kernel timeouts during long training sessions.\n\n## 7. Why We Failed\n- **Noisy Labels**: Fake/real labels may be incorrect or inconsistent.\n- **Contextual Similarity**: Texts cover similar topics with overlapping vocabulary (e.g., `new`, `mr`).\n- **Low Vocabulary Diversity**: TF-IDF produced only 870 features, indicating a limited dataset.\n\n## 8. Conclusion and Closure\n- This dataset was unsuitable for fake news detection. The 9-hour BERT training, TF-IDF models, and data analyses confirmed its limitations.\n- **Decision**: We are closing this project. A new dataset (e.g., LIAR) would be more viable, but we’re stopping here.\n- **Acknowledgment**: This process provided valuable experience in data analysis, machine learning, and deep learning. \n\n**Date**: June 4, 2025","metadata":{}}]}